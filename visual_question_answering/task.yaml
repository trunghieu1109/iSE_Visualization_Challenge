task_description:
  type: Visual Question Answering
  description: |
    This task involves answering natural language questions based on visual content provided in the form of images.
    Once the image and question are provided, the model performs inference and generates an appropriate answer grounded in the visual information presented.
  input: |
    The input consists of an image and a corresponding natural language question related to the image content.
    The question can be about objects, attributes, relationships, or actions depicted in the image.
  output: |
    The output is a natural language answer that correctly responds to the question based on the visual information in the image.

visualize:
  description: |
    Display a list of questions along with the corresponding images and predicted answers. Each item includes:
      - The input image.
      - The question related to the image.
      - The predicted answer generated by the model.
      - Confidence scores or probabilities for possible answers.
  features:
    - list_display:
        description: Show a list of question-image pairs with their prediction results.
        fields:
          - input_image: The image associated with the question.
          - question: The natural language question about the image.
          - predicted_answer: The model's predicted answer to the question.
          - answer_confidence: Confidence scores or probabilities for possible answers.
    - input_function:
        description: Allow users to upload an image and enter a question for VQA prediction.
        steps:
          - Upload or select an image.
          - Enter a natural language question about the image.
          - Display the predicted answer along with confidence scores.

model_information:
  api_url: "http://34.142.220.207:8008/api/visual-question-answering"
  name: openbmb/MiniCPM-V
  description: |
    MiniCPM-V (i.e., OmniLMM-3B) is an efficient version with promising performance for deployment. 
    The model is built based on SigLip-400M and MiniCPM-2.4B, connected by a perceiver resampler.
  input_format: 
    type: json
    structure:
      image_data:
        type: base64
        encoding: UTF-8
        description: The input is a base64 encoded image.
      query: 
        type: string
        description: The question related to information from image
  output_format: 
    type: string
    description: The answer for corresponding question
  parameters:
    max_position_embeddings: 32768
    vocab_size: 151700
    image_size: 448

dataset_description:
  description: |
    RLAIF-V-Dataset is a large-scale multimodal feedback dataset. 
    The dataset provides high-quality feedback with a total number of 83,132 preference pairs, 
    where the instructions are collected from a diverse range of datasets including MSCOCO, 
    ShareGPT-4V, MovieNet, Google Landmark v2, VQA v2, OKVQA, and TextVQA.
  data_source: ./data/rlahf_v_dataset.parquet
  data_format:
    ds_name: Name of the dataset source (e.g., RLHF-V-Dataset).
    image: 
      format: json
      structure:
        bytes: Image content in bytes format (e.g., encoded as JPEG or PNG)
    text: 
      format: json
      structure:
        question: A natural language question referring to the image.
        chosen: The suitable answer for the question.
        rejected: Rejected response for the question.
    origin_dataset: Name of the original vision dataset the image is from (e.g., coco).
    idx: Unique identifier for the data sample.
