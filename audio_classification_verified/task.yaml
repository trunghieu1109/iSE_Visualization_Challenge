task_description:
  type: Audio Classification
  description: |
    This task focuses on detecting and classifying the emotional state of a speaker from an audio recording. 
    The system analyzes vocal cues—such as tone, pitch, intensity, and rhythm—to determine which emotion the speaker is expressing. 
    The goal is to categorize the audio into one of several predefined emotional labels (e.g., "sad", "happy", "angry", "neutral", "disgust", "fearful").
  input: |
    An audio clip containing a spoken utterance or vocal segment from a single speaker. 
    The recording should include clear vocal characteristics that reflect emotional expression. 
  output: |
    A single emotion label representing the predicted emotional state of the speaker. 
    The label is one of the following: "happy", "sad", "angry", "neutral", "disgust" or "fearful". 
    Optionally, a confidence score may be provided for each emotion to indicate prediction certainty.

  visualize:
    description: |
      Display a list of input and corresponding output pairs. Each data item includes:
        - The audio clip containing a short speech from a speaker.
        - The emotion labels (e.g., "sad", "happy", "angry", "neutral", "disgust", "fearful") and their scores (probabilities).
        - An emoji corresponding to the predicted emotion.
    features:
      - list_display:
          description: Show a list of audio clips and their inferred emotion results.
          fields:
            - audio_clip: The input audio file.
            - predicted_emotions: The emotion labels with associated probabilities.
            - emotion_emoji: Emoji corresponding to the predicted dominant emotion.
      - input_function:
          description: Allow users to upload their own audio files (wav) for emotion inference.
          steps:
            - Upload an audio file (wav).
            - Display the inferred emotions with probabilities and emoji.

model_information:
  api_url: "http://34.87.113.245:8000/api/audio-classification"
  name: antonjaragon/emotions_6_classes_small
  description: This model is a fine-tuned version of facebook/wav2vec2-base on the 'Audio emotions' public dataset, available form https://www.kaggle.com/datasets/uldisvalainis/audio-emotions.
  input_format: 
    type: json
    description: Audio data and sampling rate read from audio file by using some popular libraries, such as librosa, soundfile.
    structure:
      audio_data: 
        type: List[float]
        description: Audio data read from audio file
      sampling_rate:
        type: int
        description: Sampling rate is the number of audio samples captured per second when converting an analog audio signal into a digital one.
  output_format: 
    type: List[dict]
    description: A list of dict contains emotions (labels) and their corresponding scores (probabilities).
    structure:
      label: string
      score: float

dataset_description:
  description: The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, disgust expressions and neutral.
  data_source: .wav file in folders ./data/Actor_..../.
  data_format: .wav file.
  additional_description: |
    - Dataset contains lots of speakers corresponding to one folder. Each folder includes wav files, which is a short speech segment or vocal expression from a speaker.
